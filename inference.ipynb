{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b01b0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "import joblib, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class WindTurbineInference:\n",
    "    \"\"\"\n",
    "    EXACT mirror of training, but anchored at a user-selected source point:\n",
    "      - history is trimmed up to source point\n",
    "      - builds a 3-day (288 steps @15min) lookback ending at source\n",
    "      - forecasts next 2 days (192 steps @15min)\n",
    "      - attaches actuals over the forecast window\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_path='improved_model.h5',\n",
    "                 feature_scaler_path='feature_scaler.pkl',\n",
    "                 target_scaler_path='target_scaler.pkl',\n",
    "                 features_path='common_features.json',\n",
    "                 label_encoder_path='label_encoder_alarm_system.pkl',\n",
    "                 lookback_steps=288,        # 3 days * 24 * 4 (15-min)\n",
    "                 forecast_steps=192,        # 2 days * 24 * 4 (15-min)\n",
    "                 critical_temp=70.0):\n",
    "        print(\"üîÑ Loading model & scalers...\")\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.feature_scaler: RobustScaler = joblib.load(feature_scaler_path)\n",
    "        self.target_scaler: RobustScaler = joblib.load(target_scaler_path)\n",
    "        with open(features_path, 'r') as f:\n",
    "            self.common_features = json.load(f)\n",
    "        try:\n",
    "            self.label_encoder: LabelEncoder = joblib.load(label_encoder_path)\n",
    "        except Exception:\n",
    "            self.label_encoder = None\n",
    "            print(\"‚ö†Ô∏è Label encoder not found; will fit/approximate if needed.\")\n",
    "\n",
    "        # Guard: feature order must match scaler, if available\n",
    "        scaler_feats = getattr(self.feature_scaler, \"feature_names_in_\", None)\n",
    "        if scaler_feats is not None and list(scaler_feats) != list(self.common_features):\n",
    "            raise ValueError(\"Feature order mismatch between feature_scaler and common_features.json.\")\n",
    "\n",
    "        self.lookback_steps = lookback_steps\n",
    "        self.forecast_steps = forecast_steps\n",
    "        self.critical_temp = critical_temp\n",
    "        print(\"‚úÖ Inference pipeline ready\")\n",
    "\n",
    "    # ------------------- PUBLIC API ------------------- #\n",
    "    def predict_from_point(self, latest_point, history, actual_df=None):\n",
    "        \"\"\"\n",
    "        Build a forecast that STARTS RIGHT AFTER the chosen source point.\n",
    "\n",
    "        latest_point: dict or 1-row DataFrame with 'date_time' + raw sensors\n",
    "        history: DataFrame or CSV path (5-min-ish raw)\n",
    "        actual_df: optional DataFrame to attach actual temperatures\n",
    "        \"\"\"\n",
    "        # --- 0) Load raw history --- #\n",
    "        if isinstance(history, str):\n",
    "            raw = pd.read_csv(history, parse_dates=['date_time'])\n",
    "        else:\n",
    "            raw = history.copy()\n",
    "        raw['date_time'] = pd.to_datetime(raw['date_time'])\n",
    "\n",
    "        # latest_point as DataFrame and get source timestamp\n",
    "        latest_df = pd.DataFrame([latest_point]) if isinstance(latest_point, dict) else latest_point.copy()\n",
    "        latest_df['date_time'] = pd.to_datetime(latest_df['date_time'])\n",
    "        if len(latest_df) != 1:\n",
    "            raise ValueError(\"latest_point must be a single row.\")\n",
    "        source_ts = latest_df['date_time'].iloc[0]\n",
    "\n",
    "        # --- 1) Trim history up to source point + ensure source row present --- #\n",
    "        df = raw[raw['date_time'] <= source_ts]\n",
    "        df = pd.concat([df, latest_df], ignore_index=True).drop_duplicates(subset='date_time', keep='last')\n",
    "        df = df.sort_values('date_time')\n",
    "        print(f\"üîÑ Initial trimmed shape: {df.shape}\")\n",
    "\n",
    "        # --- 2) Reindex to full 5-min grid (as in training) --- #\n",
    "        full5 = pd.date_range(df['date_time'].min(), df['date_time'].max(), freq='5min')\n",
    "        df = df.set_index('date_time').reindex(full5).reset_index().rename(columns={'index':'date_time'})\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        print(f\"‚úÖ After 5-min reindex: {df.shape}\")\n",
    "\n",
    "        # --- 3) Encode alarm_system BEFORE 15-min resample (as in training) --- #\n",
    "        if 'alarm_system' in df.columns:\n",
    "            df['alarm_system'] = df['alarm_system'].astype(str)\n",
    "            if self.label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder().fit(df['alarm_system'])\n",
    "            else:\n",
    "                # map unknowns to first known class\n",
    "                known = set(self.label_encoder.classes_)\n",
    "                if not set(df['alarm_system']).issubset(known):\n",
    "                    df['alarm_system'] = df['alarm_system'].apply(\n",
    "                        lambda x: x if x in known else list(known)[0]\n",
    "                    )\n",
    "            df['alarm_system'] = self.label_encoder.transform(df['alarm_system'])\n",
    "\n",
    "        # --- 4) Drop rows with missing alarm_desc if your training did so --- #\n",
    "        if 'alarm_desc' in df.columns:\n",
    "            df = df.dropna(subset=['alarm_desc'])\n",
    "\n",
    "        # --- 5) Resample to 15-min (numerics mean, alarm max) --- #\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        df_num = df[['date_time'] + num_cols].set_index('date_time').resample('15min').mean()\n",
    "        if 'alarm_system' in df.columns:\n",
    "            alarm_15 = df[['date_time','alarm_system']].set_index('date_time').resample('15min').max()\n",
    "            df_num['alarm_system'] = alarm_15['alarm_system']\n",
    "        df_15 = df_num.reset_index()\n",
    "\n",
    "        # --- 6) Feature engineering (same as training) --- #\n",
    "        df_15 = self._alarm_features(df_15)\n",
    "        df_15 = self._enhanced_features(df_15)\n",
    "\n",
    "        # --- 7) Align to source grid time and ensure feature columns --- #\n",
    "        source_15 = pd.Timestamp(source_ts).floor('15min')\n",
    "        df_15 = df_15[df_15['date_time'] <= source_15].reset_index(drop=True)\n",
    "\n",
    "        for f in self.common_features:\n",
    "            if f not in df_15.columns:\n",
    "                df_15[f] = 0.0\n",
    "        # keep only needed cols in correct order\n",
    "        df_clean = df_15[self.common_features + ['wtrm_avg_TrmTmp_Gbx', 'date_time']].dropna().reset_index(drop=True)\n",
    "        print(f\"‚úÖ Final preprocessed shape (<= source): {df_clean.shape}\")\n",
    "\n",
    "        # --- 8) Build last 3-day lookback ending at source_15 --- #\n",
    "        X, anchor_time = self._sequence_from_tail(df_clean)\n",
    "        # If padding occurred, anchor_time is still the last row (source_15).\n",
    "\n",
    "        # --- 9) Scale features (train scaler, order safe) --- #\n",
    "        X_scaled = self._scale_features(X).astype(np.float32)\n",
    "\n",
    "        # --- 10) Predict next 2 days --- #\n",
    "        y_scaled = self.model.predict(X_scaled, verbose=0)\n",
    "        if y_scaled.ndim == 3:\n",
    "            y_scaled = y_scaled.reshape(y_scaled.shape[0], -1)\n",
    "        # inverse transform expects shape (1, forecast_steps)\n",
    "        y = self.target_scaler.inverse_transform(y_scaled)[0]\n",
    "\n",
    "        # --- 11) Forecast timestamps start right after source_15 --- #\n",
    "        forecast_times = pd.date_range(\n",
    "            start=source_15 + pd.Timedelta(minutes=15),\n",
    "            periods=len(y),\n",
    "            freq='15min'\n",
    "        )\n",
    "        preds_df = pd.DataFrame({'timestamp': forecast_times, 'predicted_temperature': y})\n",
    "\n",
    "        # --- 12) Threshold & summary stats --- #\n",
    "        exceed_mask = y > self.critical_temp\n",
    "        exceeded = bool(exceed_mask.any())\n",
    "        max_idx = int(np.argmax(y))\n",
    "        max_temp = float(y[max_idx])\n",
    "        max_time = forecast_times[max_idx]\n",
    "        first_exceed_time = forecast_times[np.argmax(exceed_mask)] if exceeded else None\n",
    "        total_exceed_count = int(exceed_mask.sum())\n",
    "\n",
    "        # --- 13) Attach actuals within forecast window --- #\n",
    "        preds_with_actuals, overlap_metrics = self._attach_actuals(preds_df, actual_df)\n",
    "\n",
    "        # --- 14) Package results --- #\n",
    "        results = {\n",
    "            'predictions': preds_with_actuals,\n",
    "            'exceeded': exceeded,\n",
    "            'first_exceed_time': first_exceed_time,\n",
    "            'max_temperature': max_temp,\n",
    "            'max_temperature_time': max_time,\n",
    "            'critical_temperature_threshold': self.critical_temp,\n",
    "            'prediction_start': source_15,\n",
    "            'prediction_end': forecast_times[-1],\n",
    "            'total_exceed_count': total_exceed_count,\n",
    "            'overlap_metrics': overlap_metrics\n",
    "        }\n",
    "\n",
    "        # --- 15) Pretty log --- #\n",
    "        self._log_summary(results, source_ts)\n",
    "\n",
    "        # sanity check\n",
    "        assert len(y) == self.forecast_steps, \"Forecast length mismatch vs forecast_steps.\"\n",
    "        return results\n",
    "\n",
    "    # ------------------- HELPERS (mirror training) ------------------- #\n",
    "    def _alarm_features(self, df, alarm_col='alarm_system', time_col='date_time'):\n",
    "        df = df.copy()\n",
    "        if alarm_col in df.columns:\n",
    "            alarms = df[df[alarm_col] == 1][time_col].reset_index(drop=True)\n",
    "\n",
    "            def hours_since(ts):\n",
    "                past = alarms[alarms < ts]\n",
    "                return (ts - past.iloc[-1]).total_seconds()/3600 if not past.empty else np.nan\n",
    "\n",
    "            df['hours_since_last_alarm'] = df[time_col].apply(hours_since).fillna(48)\n",
    "            df['recent_alarm_flag'] = (df['hours_since_last_alarm'] < 6).astype(int)\n",
    "            df['alarm_frequency_24h'] = df[alarm_col].rolling(window=96).sum().fillna(0)  # 24h at 15-min\n",
    "            df['alarm_system_lag_0.5h'] = df[alarm_col].shift(1)\n",
    "            df['alarm_system_lag_2h']   = df[alarm_col].shift(4)\n",
    "        else:\n",
    "            df['hours_since_last_alarm'] = 48.0\n",
    "            df['recent_alarm_flag'] = 0\n",
    "            df['alarm_frequency_24h'] = 0.0\n",
    "            df['alarm_system_lag_0.5h'] = 0\n",
    "            df['alarm_system_lag_2h'] = 0\n",
    "        return df\n",
    "\n",
    "    def _enhanced_features(self, df, target_col='wtrm_avg_TrmTmp_Gbx', time_col='date_time'):\n",
    "        df = df.copy()\n",
    "\n",
    "        # Critical temps with lags/deltas\n",
    "        critical = ['wtrm_avg_TrmTmp_GbxBrg452','wtrm_avg_TrmTmp_GbxBrg151','wtrm_avg_TrmTmp_Gbx']\n",
    "        lag_steps = [1,2,4,8,16]  # 0.5h,1h,2h,4h,8h\n",
    "        for col in critical:\n",
    "            if col in df.columns:\n",
    "                for lag in lag_steps:\n",
    "                    df[f'{col}_lag_{lag*0.5}h'] = df[col].shift(lag)\n",
    "                df[f'{col}_delta_1h'] = df[col] - df[col].shift(2)\n",
    "                df[f'{col}_delta_4h'] = df[col] - df[col].shift(8)\n",
    "\n",
    "        if target_col in df.columns:\n",
    "            df[f'{target_col}_rolling_mean_3.0h'] = df[target_col].rolling(window=6).mean()\n",
    "\n",
    "        # Ops features\n",
    "        for col in ['wgen_avg_Spd','wgdc_avg_TriGri_PwrAt','wtrm_avg_Gbx_OilPres']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_delta_1h'] = df[col] - df[col].shift(2)\n",
    "                df[f'{col}_rolling_mean_6h'] = df[col].rolling(window=12).mean()\n",
    "\n",
    "        # Time features\n",
    "        df['hour'] = df[time_col].dt.hour + df[time_col].dt.minute/60\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "        df['day_of_week'] = df[time_col].dt.dayofweek\n",
    "\n",
    "        df['week_of_year'] = df[time_col].dt.isocalendar().week.astype(int)\n",
    "        df['week_sin'] = np.sin(2*np.pi*df['week_of_year']/52)\n",
    "        df['week_cos'] = np.cos(2*np.pi*df['week_of_year']/52)\n",
    "\n",
    "        df['month'] = df[time_col].dt.month\n",
    "        df['month_sin'] = np.sin(2*np.pi*df['month']/12)\n",
    "        df['month_cos'] = np.cos(2*np.pi*df['month']/12)\n",
    "\n",
    "        # Clean\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def _sequence_from_tail(self, df):\n",
    "        \"\"\"Take last lookback_steps (pad if needed) ending at the last row (source_15).\"\"\"\n",
    "        if len(df) < self.lookback_steps:\n",
    "            pad_needed = self.lookback_steps - len(df)\n",
    "            last_row = df.iloc[-1:].copy()\n",
    "            end_time = df['date_time'].iloc[-1]\n",
    "            pad_times = pd.date_range(end=end_time - pd.Timedelta(minutes=15),\n",
    "                                      periods=pad_needed, freq='-15min')[::-1]\n",
    "            pad_df = pd.concat([last_row]*pad_needed, ignore_index=True)\n",
    "            pad_df['date_time'] = pad_times\n",
    "            df = pd.concat([pad_df, df], ignore_index=True)\n",
    "\n",
    "        feat = df[self.common_features].values[-self.lookback_steps:]\n",
    "        anchor_time = df['date_time'].iloc[-1]  # == source_15\n",
    "        X = feat.reshape(1, self.lookback_steps, len(self.common_features))\n",
    "        return X, anchor_time\n",
    "\n",
    "    def _scale_features(self, X):\n",
    "        \"\"\"Transform with training feature_scaler (order already validated).\"\"\"\n",
    "        X2d = X.reshape(-1, X.shape[-1])\n",
    "        # if scaler has feature_names_in_, ensure same order:\n",
    "        scaler_feats = getattr(self.feature_scaler, \"feature_names_in_\", None)\n",
    "        if scaler_feats is not None and list(scaler_feats) != list(self.common_features):\n",
    "            raise ValueError(\"Feature order drift before scaling.\")\n",
    "        Xs = self.feature_scaler.transform(X2d).reshape(X.shape)\n",
    "        return Xs\n",
    "\n",
    "    def _attach_actuals(self, preds_df, actual_df, target_col='wtrm_avg_TrmTmp_Gbx'):\n",
    "        \"\"\"Attach actuals over the forecast window; robust to small timing drift.\"\"\"\n",
    "        if actual_df is None or len(actual_df) == 0:\n",
    "            return preds_df, {}\n",
    "\n",
    "        df = actual_df.copy()\n",
    "        if 'date_time' not in df.columns or target_col not in df.columns:\n",
    "            return preds_df, {}\n",
    "\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')\n",
    "        df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "        df = df.dropna(subset=['date_time'])\n",
    "\n",
    "        # align to 15-min grid and average duplicates\n",
    "        df['date_time'] = df['date_time'].dt.floor('15min')\n",
    "        actuals_15 = (df[['date_time', target_col]]\n",
    "                        .groupby('date_time', as_index=False)\n",
    "                        .mean()\n",
    "                        .rename(columns={'date_time':'timestamp', target_col:'actual_temperature'}))\n",
    "\n",
    "        # limit to forecast window\n",
    "        start_ts = preds_df['timestamp'].min()\n",
    "        end_ts   = preds_df['timestamp'].max()\n",
    "        window_actuals = actuals_15[(actuals_15['timestamp'] >= start_ts) &\n",
    "                                    (actuals_15['timestamp'] <= end_ts)].copy()\n",
    "\n",
    "        preds = preds_df.sort_values('timestamp').copy()\n",
    "        merged = preds.merge(window_actuals, on='timestamp', how='left')\n",
    "        exact_cover = merged['actual_temperature'].notna().mean()\n",
    "\n",
    "        # fallback nearest-with-tolerance if coverage poor\n",
    "        if exact_cover < 0.8:\n",
    "            merged = pd.merge_asof(\n",
    "                preds.sort_values('timestamp'),\n",
    "                window_actuals.sort_values('timestamp'),\n",
    "                on='timestamp',\n",
    "                direction='nearest',\n",
    "                tolerance=pd.Timedelta(minutes=7, seconds=30)\n",
    "            )\n",
    "\n",
    "        mask = merged['actual_temperature'].notna()\n",
    "        metrics = {}\n",
    "        if mask.any():\n",
    "            y_true = merged.loc[mask, 'actual_temperature'].to_numpy()\n",
    "            y_pred = merged.loc[mask, 'predicted_temperature'].to_numpy()\n",
    "            mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "            rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "            bias = float(np.mean(y_pred - y_true))\n",
    "            metrics = {'overlap_points': int(mask.sum()), 'mae': mae, 'rmse': rmse, 'bias': bias}\n",
    "            merged.loc[mask, 'abs_error'] = np.abs(y_true - y_pred)\n",
    "        else:\n",
    "            merged['abs_error'] = np.nan\n",
    "\n",
    "        return merged, metrics\n",
    "\n",
    "    def _log_summary(self, results, source_ts):\n",
    "        print(\"\\nüìä Inference Summary\")\n",
    "        print(f\"üìç Source point timestamp: {pd.Timestamp(source_ts)}\")\n",
    "        print(f\"üîÆ Forecast window: {results['prediction_start']} ‚Üí {results['prediction_end']}\")\n",
    "        print(f\"üå°Ô∏è Peak temperature: {results['max_temperature']:.2f}¬∞C at {results['max_temperature_time']}\")\n",
    "        if results['exceeded']:\n",
    "            # print(f\"‚ö†Ô∏è Threshold {results['critical_temperature_threshold']}¬∞C exceeded {results['total_exceed_count']} times\")\n",
    "            print(f\"   First exceedance: {results['first_exceed_time']}\")\n",
    "        if results['overlap_metrics']:\n",
    "            m = results['overlap_metrics']\n",
    "            print(f\"üîÑ Overlap: {m['overlap_points']} points | MAE={m['mae']:.3f}¬∞C, RMSE={m['rmse']:.3f}¬∞C, Bias={m['bias']:.3f}¬∞C\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85d60867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model & scalers...\n",
      "‚úÖ Inference pipeline ready\n",
      "üîÑ Initial trimmed shape: (27384, 319)\n",
      "‚úÖ After 5-min reindex: (27649, 319)\n",
      "‚úÖ Final preprocessed shape (<= source): (9217, 23)\n",
      "\n",
      "üìä Inference Summary\n",
      "üìç Source point timestamp: 2012-04-06 01:05:00\n",
      "üîÆ Forecast window: 2012-04-06 01:00:00 ‚Üí 2012-04-08 01:00:00\n",
      "üå°Ô∏è Peak temperature: 65.34¬∞C at 2012-04-06 01:15:00\n",
      "   First exceedance: 2012-04-06 01:15:00\n",
      "üîÑ Overlap: 192 points | MAE=7.487¬∞C, RMSE=9.019¬∞C, Bias=-4.231¬∞C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------- USAGE EXAMPLE ------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    # Load some recent chunk\n",
    "    history_df = pd.read_csv(\"wt84_with_alarms.csv\", parse_dates=[\"date_time\"])\n",
    "\n",
    "    # Choose a source point (any row you want to forecast after)\n",
    "    anchor_idx = len(history_df) // 2\n",
    "    chosen_time = \"2012-04-06 01:05:00\"  # your desired source point\n",
    "    latest_point = history_df.loc[history_df['date_time'] == chosen_time].to_dict(orient='records')[0]\n",
    "\n",
    "    pipe = WindTurbineInference(\n",
    "        lookback_steps=384,   # 4 days\n",
    "        forecast_steps=192,   # 2 days\n",
    "        critical_temp=65.0\n",
    "    )\n",
    "\n",
    "    out = pipe.predict_from_point(\n",
    "        latest_point=latest_point,\n",
    "        history=history_df,\n",
    "        actual_df=history_df  # optional, for metrics\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b74347ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Head of predictions:\n",
      "             timestamp  predicted_temperature  actual_temperature  abs_error\n",
      "0  2012-04-06 01:15:00              65.344971           70.400000   5.055029\n",
      "1  2012-04-06 01:30:00              65.275948           68.733333   3.457386\n",
      "2  2012-04-06 01:45:00              65.188950           67.733333   2.544384\n",
      "3  2012-04-06 02:00:00              65.008347           69.400000   4.391653\n",
      "4  2012-04-06 02:15:00              65.131577           70.400000   5.268423\n",
      "5  2012-04-06 02:30:00              64.419289           71.400000   6.980711\n",
      "6  2012-04-06 02:45:00              64.735710           72.066667   7.330957\n",
      "7  2012-04-06 03:00:00              64.703072           72.400000   7.696928\n",
      "8  2012-04-06 03:15:00              64.239281           72.400000   8.160719\n",
      "9  2012-04-06 03:30:00              64.009682           72.400000   8.390318\n",
      "10 2012-04-06 03:45:00              63.795387           73.400000   9.604613\n",
      "11 2012-04-06 04:00:00              64.028893           73.400000   9.371107\n",
      "12 2012-04-06 04:15:00              63.447929           73.400000   9.952071\n",
      "13 2012-04-06 04:30:00              63.363121           73.400000  10.036879\n",
      "14 2012-04-06 04:45:00              63.463848           73.400000   9.936152\n",
      "15 2012-04-06 05:00:00              63.019215           71.398633   8.379419\n",
      "16 2012-04-06 05:15:00              63.008545           66.677267   3.668722\n",
      "17 2012-04-06 05:30:00              62.615238           63.733333   1.118095\n",
      "18 2012-04-06 05:45:00              62.485973           64.467133   1.981160\n",
      "19 2012-04-06 06:00:00              62.567097           68.400000   5.832903\n",
      "\n",
      "üìä Overlap metrics: {'overlap_points': 192, 'mae': 7.487123580731288, 'rmse': 9.018894797516676, 'bias': -4.2306635182168755}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüìã Head of predictions:\")\n",
    "print(out['predictions'].head(20))\n",
    "# print(out['predictions'].tail(10))\n",
    "print(\"\\nüìä Overlap metrics:\", out['overlap_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b27a68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Predicted exceedances > 65.0¬∞C: 31\n",
      "üîç Actual exceedances > 70.0¬∞C: 56\n"
     ]
    }
   ],
   "source": [
    "# After running the pipeline\n",
    "pred_df = out['predictions']\n",
    "\n",
    "# Define limits\n",
    "predicted_limit = 65.0\n",
    "actual_limit = 70.0\n",
    "\n",
    "# Count predicted exceedances\n",
    "predicted_exceed_count = (pred_df['predicted_temperature'] > predicted_limit).sum()\n",
    "\n",
    "# Count actual exceedances (only where actual data is available)\n",
    "if 'actual_temperature' in pred_df.columns:\n",
    "    actual_exceed_count = (pred_df['actual_temperature'] > actual_limit).sum()\n",
    "else:\n",
    "    actual_exceed_count = None\n",
    "\n",
    "# Print results\n",
    "print(f\"üîç Predicted exceedances > {predicted_limit}¬∞C: {predicted_exceed_count}\")\n",
    "if actual_exceed_count is not None:\n",
    "    print(f\"üîç Actual exceedances > {actual_limit}¬∞C: {actual_exceed_count}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No actual data available to count exceedances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e161b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearbox-lstm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
