{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f088624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "import joblib, json, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class WindTurbineInference:\n",
    "    \"\"\"\n",
    "    Inference pipeline that mirrors the training pipeline:\n",
    "    - Accepts one latest measurement (single point) + history\n",
    "    - Reconstructs exact preprocessing/feature engineering\n",
    "    - Builds last lookback window and forecasts\n",
    "    - Returns predictions and threshold summary\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_path='improved_model.h5',\n",
    "                 feature_scaler_path='feature_scaler.pkl',\n",
    "                 target_scaler_path='target_scaler.pkl',\n",
    "                 features_path='common_features.json',\n",
    "                 label_encoder_path='label_encoder_alarm_system.pkl',\n",
    "                 lookback_steps=384,   # 384 * 15min = 96 hours lookback\n",
    "                 forecast_steps=192,   # 192 * 15min = 48 hours forecast\n",
    "                 critical_temp=70.0):\n",
    "        print(\"üîÑ Loading model & scalers...\")\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.feature_scaler: RobustScaler = joblib.load(feature_scaler_path)\n",
    "        self.target_scaler: RobustScaler = joblib.load(target_scaler_path)\n",
    "        with open(features_path, 'r') as f:\n",
    "            self.common_features = json.load(f)\n",
    "        try:\n",
    "            self.label_encoder: LabelEncoder = joblib.load(label_encoder_path)\n",
    "        except Exception:\n",
    "            self.label_encoder = None\n",
    "            print(\"‚ö†Ô∏è Label encoder not found; will create on-the-fly if needed.\")\n",
    "        self.lookback_steps = lookback_steps\n",
    "        self.forecast_steps = forecast_steps\n",
    "        self.critical_temp = critical_temp\n",
    "        print(\"‚úÖ Ready.\")\n",
    "\n",
    "    # ------------ Public API ------------ #\n",
    "\n",
    "    def predict_from_point(self, latest_point, history, history_freq='5min', actual_df=None):\n",
    "        \"\"\"\n",
    "        Build a forecast from the latest point + recent history while mirroring the training pipeline.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        latest_point : dict or one-row DataFrame\n",
    "            Must contain 'date_time' and raw sensor fields.\n",
    "        history : DataFrame or CSV path\n",
    "            Recent raw data (ideally 5-min grid), used to reconstruct lookback features.\n",
    "        history_freq : str\n",
    "            Frequency of the history data ('5min' recommended).\n",
    "        actual_df : DataFrame or None\n",
    "            If provided, will be aligned (15-min) and merged to show 'actual_temperature' alongside\n",
    "            predictions, and quick overlap metrics will be returned.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict with:\n",
    "            - predictions (DataFrame): timestamp | predicted_temperature | [actual_temperature] | [abs_error]\n",
    "            - exceeded (bool)\n",
    "            - first_exceed_time (Timestamp or None)\n",
    "            - max_temperature (float)\n",
    "            - max_temperature_time (Timestamp)\n",
    "            - critical_temperature_threshold (float)\n",
    "            - prediction_start (Timestamp)\n",
    "            - prediction_end (Timestamp)\n",
    "            - total_exceed_count (int)\n",
    "            - overlap_metrics (dict)  # present only if actuals overlap\n",
    "        \"\"\"\n",
    "        # --- Load & normalize inputs ---\n",
    "        if isinstance(history, str):\n",
    "            hist_df = pd.read_csv(history, parse_dates=['date_time'])\n",
    "        else:\n",
    "            hist_df = history.copy()\n",
    "        hist_df['date_time'] = pd.to_datetime(hist_df['date_time'])\n",
    "\n",
    "        latest_df = self._to_dataframe(latest_point)\n",
    "        latest_df['date_time'] = pd.to_datetime(latest_df['date_time'])\n",
    "\n",
    "        # Concatenate history + latest row\n",
    "        raw = pd.concat([hist_df, latest_df], ignore_index=True).sort_values('date_time')\n",
    "\n",
    "        # Keep just what we need (lookback + buffer) to save time/memory\n",
    "        cutoff = raw['date_time'].max() - pd.Timedelta(days=15)\n",
    "        raw = raw[raw['date_time'] >= cutoff].reset_index(drop=True)\n",
    "\n",
    "        # --- Preprocess to 15-min grid (same as training) ---\n",
    "        df_15 = self._preprocess_to_15min(raw, history_freq)\n",
    "\n",
    "        # --- Feature engineering (exactly as training) ---\n",
    "        df_feat = self._alarm_features(df_15)\n",
    "        df_feat = self._enhanced_features(df_feat)\n",
    "\n",
    "        # Ensure every expected feature exists (fill missing with 0.0)\n",
    "        for f in self.common_features:\n",
    "            if f not in df_feat.columns:\n",
    "                df_feat[f] = 0.0\n",
    "\n",
    "        # --- Build final sequence from the tail, pad if needed ---\n",
    "        X, pred_anchor_time = self._sequence_from_tail(df_feat, self.common_features)\n",
    "\n",
    "        # --- Scale with training feature scaler ---\n",
    "        X_scaled = self._scale_features(X)\n",
    "\n",
    "        # --- Predict ---\n",
    "        y_scaled = self.model.predict(X_scaled, verbose=0)\n",
    "        if y_scaled.ndim == 3:\n",
    "            y_scaled = y_scaled.reshape(y_scaled.shape[0], -1)\n",
    "        y = self.target_scaler.inverse_transform(y_scaled)[0]\n",
    "\n",
    "        # --- Build forecast timestamps (15-min steps) ---\n",
    "        prediction_times = pd.date_range(\n",
    "            start=pd.Timestamp(pred_anchor_time) + pd.Timedelta(minutes=15),\n",
    "            periods=len(y),\n",
    "            freq='15min'\n",
    "        )\n",
    "        preds_df = pd.DataFrame({\n",
    "            'timestamp': prediction_times,\n",
    "            'predicted_temperature': y\n",
    "        })\n",
    "\n",
    "        # --- Threshold summary ---\n",
    "        max_idx = int(np.argmax(y))\n",
    "        max_temp = float(y[max_idx])\n",
    "        max_time = prediction_times[max_idx]\n",
    "        exceed_mask = y > self.critical_temp\n",
    "        exceeded = bool(exceed_mask.any())\n",
    "        first_exceed_time = prediction_times[np.argmax(exceed_mask)] if exceeded else None\n",
    "        total_exceed_count = int(exceed_mask.sum())\n",
    "\n",
    "        # --- Attach actuals (if provided) + quick metrics ---\n",
    "        preds_with_actuals, overlap_metrics = self._attach_actuals_and_metrics(\n",
    "            preds_df, actual_df, target_col='wtrm_avg_TrmTmp_Gbx'\n",
    "        )\n",
    "\n",
    "        # --- Package results ---\n",
    "        summary = {\n",
    "            'predictions': preds_with_actuals,\n",
    "            'exceeded': exceeded,\n",
    "            'first_exceed_time': first_exceed_time,\n",
    "            'max_temperature': max_temp,\n",
    "            'max_temperature_time': max_time,\n",
    "            'critical_temperature_threshold': self.critical_temp,\n",
    "            'prediction_start': pd.Timestamp(pred_anchor_time),\n",
    "            'prediction_end': prediction_times[-1],\n",
    "            'total_exceed_count': total_exceed_count,\n",
    "            'overlap_metrics': overlap_metrics\n",
    "        }\n",
    "\n",
    "        # --- Pretty log ---\n",
    "        print(\"\\nüìä Inference Summary\")\n",
    "        print(f\"Source point taken at: {latest_df['date_time'].iloc[0]}\")\n",
    "        print(f\"Forecast window: {summary['prediction_start']} ‚Üí {summary['prediction_end']}\")\n",
    "        print(f\"Peak: {max_temp:.2f}¬∞C at {max_time}\")\n",
    "        if exceeded:\n",
    "            print(f\"‚ö†Ô∏è Threshold {self.critical_temp}¬∞C exceeded {total_exceed_count} times.\")\n",
    "            print(f\"   First exceed at: {first_exceed_time}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ No exceedances over {self.critical_temp}¬∞C\")\n",
    "\n",
    "        if overlap_metrics:\n",
    "            print(f\"üîÅ Overlap with actuals: {overlap_metrics['overlap_points']} points\")\n",
    "            print(f\"   MAE={overlap_metrics['mae']:.3f}¬∞C, RMSE={overlap_metrics['rmse']:.3f}¬∞C, Bias={overlap_metrics['bias']:.3f}¬∞C\")\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "    # ------------ Helpers (mirror training) ------------ #\n",
    "\n",
    "    def _to_dataframe(self, latest_point):\n",
    "        if isinstance(latest_point, dict):\n",
    "            return pd.DataFrame([latest_point])\n",
    "        df = latest_point.copy()\n",
    "        if len(df) != 1:\n",
    "            raise ValueError(\"latest_point must be a single row.\")\n",
    "        return df\n",
    "\n",
    "    def _preprocess_to_15min(self, df, history_freq='5min'):\n",
    "        df = df.copy()\n",
    "        df = df.sort_values('date_time')\n",
    "        # Drop exact duplicates\n",
    "        df = df.drop_duplicates(subset='date_time', keep='last')\n",
    "\n",
    "        # Fill gaps on 5-min grid if history is 5-min (matches training)\n",
    "        if history_freq == '5min':\n",
    "            full5 = pd.date_range(df['date_time'].min(), df['date_time'].max(), freq='5min')\n",
    "            df = df.set_index('date_time').reindex(full5).reset_index().rename(columns={'index': 'date_time'})\n",
    "            df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # Encode alarm_system same as training\n",
    "        if 'alarm_system' in df.columns:\n",
    "            if self.label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                df['alarm_system'] = self.label_encoder.fit_transform(df['alarm_system'].astype(str))\n",
    "            else:\n",
    "                try:\n",
    "                    df['alarm_system'] = self.label_encoder.transform(df['alarm_system'].astype(str))\n",
    "                except ValueError:\n",
    "                    # map unknown labels to first known class\n",
    "                    known = set(self.label_encoder.classes_)\n",
    "                    df['alarm_system'] = df['alarm_system'].astype(str).apply(\n",
    "                        lambda x: x if x in known else self.label_encoder.classes_[0]\n",
    "                    )\n",
    "                    df['alarm_system'] = self.label_encoder.transform(df['alarm_system'])\n",
    "\n",
    "        # 15-min aggregation identical to training\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        df_num = df[['date_time'] + num_cols].set_index('date_time').resample('15min').mean()\n",
    "        if 'alarm_system' in df.columns:\n",
    "            alarm_15 = df[['date_time','alarm_system']].set_index('date_time').resample('15min').max()\n",
    "            df_num['alarm_system'] = alarm_15['alarm_system']\n",
    "        return df_num.reset_index()\n",
    "\n",
    "    def _alarm_features(self, df, alarm_col='alarm_system', time_col='date_time'):\n",
    "        df = df.copy()\n",
    "        if alarm_col in df.columns:\n",
    "            alarms = df[df[alarm_col] == 1][time_col].reset_index(drop=True)\n",
    "\n",
    "            def hours_since(ts):\n",
    "                past = alarms[alarms < ts]\n",
    "                return (ts - past.iloc[-1]).total_seconds()/3600 if not past.empty else np.nan\n",
    "\n",
    "            df['hours_since_last_alarm'] = df[time_col].apply(hours_since).fillna(48)\n",
    "            df['recent_alarm_flag'] = (df['hours_since_last_alarm'] < 6).astype(int)\n",
    "            df['alarm_frequency_24h'] = df[alarm_col].rolling(window=48).sum().fillna(0)\n",
    "            df['alarm_system_lag_0.5h'] = df[alarm_col].shift(1)\n",
    "            df['alarm_system_lag_2h']   = df[alarm_col].shift(4)\n",
    "        else:\n",
    "            df['hours_since_last_alarm'] = 48\n",
    "            df['recent_alarm_flag'] = 0\n",
    "            df['alarm_frequency_24h'] = 0\n",
    "            df['alarm_system_lag_0.5h'] = 0\n",
    "            df['alarm_system_lag_2h'] = 0\n",
    "        return df\n",
    "    \n",
    "    # Add this helper inside WindTurbineInference class\n",
    "    def _attach_actuals_and_metrics(self, preds_df, actual_df, target_col='wtrm_avg_TrmTmp_Gbx'):\n",
    "        if actual_df is None or len(actual_df) == 0:\n",
    "            return preds_df, {}\n",
    "\n",
    "        df = actual_df.copy()\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "        # Resample to 15-min mean so it aligns with the forecast grid\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if target_col not in num_cols:\n",
    "            # No target column available -> nothing to merge\n",
    "            return preds_df, {}\n",
    "\n",
    "        df15 = (df[['date_time', target_col]]\n",
    "                .set_index('date_time')\n",
    "                .resample('15min').mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={'date_time': 'timestamp',\n",
    "                                target_col: 'actual_temperature'}))\n",
    "\n",
    "        merged = preds_df.merge(df15, on='timestamp', how='left')\n",
    "\n",
    "        # Compute metrics on overlapping timestamps (where actuals exist)\n",
    "        mask = merged['actual_temperature'].notna()\n",
    "        metrics = {}\n",
    "        if mask.any():\n",
    "            y_true = merged.loc[mask, 'actual_temperature'].values\n",
    "            y_pred = merged.loc[mask, 'predicted_temperature'].values\n",
    "            mae = float(np.mean(np.abs(y_true - y_pred)))\n",
    "            rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "            bias = float(np.mean(y_pred - y_true))\n",
    "            metrics = {'overlap_points': int(mask.sum()),\n",
    "                    'mae': mae, 'rmse': rmse, 'bias': bias}\n",
    "\n",
    "            # Keep a quick column for visibility\n",
    "            merged.loc[mask, 'abs_error'] = np.abs(y_true - y_pred)\n",
    "        else:\n",
    "            merged['abs_error'] = np.nan\n",
    "\n",
    "        return merged, metrics\n",
    "\n",
    "\n",
    "    def _enhanced_features(self, df, target_col='wtrm_avg_TrmTmp_Gbx', time_col='date_time'):\n",
    "        df = df.copy()\n",
    "\n",
    "        # Lags/deltas for critical temps\n",
    "        critical = ['wtrm_avg_TrmTmp_GbxBrg452','wtrm_avg_TrmTmp_GbxBrg151','wtrm_avg_TrmTmp_Gbx']\n",
    "        lag_steps = [1,2,4,8,16]  # 0.5h,1h,2h,4h,8h at 15-min resolution\n",
    "        for s in critical:\n",
    "            if s in df.columns:\n",
    "                for lag in lag_steps:\n",
    "                    df[f'{s}_lag_{lag*0.5}h'] = df[s].shift(lag)\n",
    "                df[f'{s}_delta_1h'] = df[s] - df[s].shift(2)\n",
    "                df[f'{s}_delta_4h'] = df[s] - df[s].shift(8)\n",
    "\n",
    "        if target_col in df.columns:\n",
    "            df[f'{target_col}_rolling_mean_3.0h'] = df[target_col].rolling(window=6).mean()\n",
    "\n",
    "        # Ops features\n",
    "        for col in ['wgen_avg_Spd','wgdc_avg_TriGri_PwrAt','wtrm_avg_Gbx_OilPres']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_delta_1h'] = df[col] - df[col].shift(2)\n",
    "                df[f'{col}_rolling_mean_6h'] = df[col].rolling(window=12).mean()\n",
    "\n",
    "        # Time features\n",
    "        df['hour'] = df[time_col].dt.hour + df[time_col].dt.minute/60\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "        df['day_of_week'] = df[time_col].dt.dayofweek\n",
    "\n",
    "        df['week_of_year'] = df[time_col].dt.isocalendar().week.astype(int)\n",
    "        df['week_sin'] = np.sin(2*np.pi*df['week_of_year']/52)\n",
    "        df['week_cos'] = np.cos(2*np.pi*df['week_of_year']/52)\n",
    "\n",
    "        df['month'] = df[time_col].dt.month\n",
    "        df['month_sin'] = np.sin(2*np.pi*df['month']/12)\n",
    "        df['month_cos'] = np.cos(2*np.pi*df['month']/12)\n",
    "\n",
    "        # Final clean\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def _sequence_from_tail(self, df, feature_cols):\n",
    "        if len(df) < self.lookback_steps:\n",
    "            pad_needed = self.lookback_steps - len(df)\n",
    "            last_row = df.iloc[-1:].copy()\n",
    "            # synth pad going backwards on 15-min grid\n",
    "            end_time = df['date_time'].iloc[-1]\n",
    "            pad_times = pd.date_range(end=end_time - pd.Timedelta(minutes=15),\n",
    "                                      periods=pad_needed, freq='-15min')[::-1]\n",
    "            pad_df = pd.concat([last_row]*pad_needed, ignore_index=True)\n",
    "            pad_df['date_time'] = pad_times\n",
    "            df = pd.concat([pad_df, df], ignore_index=True)\n",
    "\n",
    "        feat = df[feature_cols].values\n",
    "        anchor_time = df['date_time'].iloc[-1]\n",
    "        X = feat[-self.lookback_steps:].reshape(1, self.lookback_steps, len(feature_cols))\n",
    "        return X, anchor_time\n",
    "\n",
    "    def _scale_features(self, X):\n",
    "        Xr = X.reshape(-1, X.shape[-1])\n",
    "        Xs = self.feature_scaler.transform(Xr).reshape(X.shape)\n",
    "        return Xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6a7e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model & scalers...\n",
      "‚úÖ Ready.\n",
      "\n",
      "üìä Inference Summary\n",
      "Source point taken at: 2014-08-24 11:35:00\n",
      "Forecast window: 2014-12-08 06:15:00 ‚Üí 2014-12-10 06:15:00\n",
      "Peak: 65.84¬∞C at 2014-12-08 23:30:00\n",
      "‚úÖ No exceedances over 70.0¬∞C\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load a recent history slice (5-min raw)\n",
    "history_df = pd.read_csv('wt84_with_alarms.csv', parse_dates=['date_time']).tail(60000)\n",
    "\n",
    "# 2) Pick an anchor in the middle (so predictions have future actuals in file)\n",
    "anchor_index = len(history_df) // 2\n",
    "latest_point = history_df.iloc[[anchor_index]].to_dict(orient='records')[0]\n",
    "\n",
    "# 3) Run inference with the full history_df as both history and actuals\n",
    "pipe = WindTurbineInference()\n",
    "out = pipe.predict_from_point(latest_point, history_df, actual_df=history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00c8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp  predicted_temperature  actual_temperature  abs_error\n",
      "0 2014-12-08 06:30:00              62.589314                 NaN        NaN\n",
      "1 2014-12-08 06:45:00              62.591877                 NaN        NaN\n",
      "2 2014-12-08 07:00:00              61.928051                 NaN        NaN\n",
      "3 2014-12-08 07:15:00              61.889572                 NaN        NaN\n",
      "4 2014-12-08 07:30:00              61.736004                 NaN        NaN\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Show results\n",
    "print(out['predictions'].head())  # will now have actual_temperature + abs_error\n",
    "print(out['overlap_metrics'])     # MAE, RMSE, bias, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f286d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearbox-lstm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
